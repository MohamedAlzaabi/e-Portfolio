<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Research Methods - Mohamed Alzaabi</title>
  <link rel="stylesheet" href="style.css">
  <link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" />
</head>
<body>
  <!-- Header Section -->
  <header class="header">
    <div class="logo-left">
      <span>University of Essex</span>
    </div>

    <nav class="navbar">
      <ul class="nav-menu">
        <li><a href="index.html">Home</a></li>
        <li><a href="AboutMe.html">About Me</a></li>
        <li><a href="MyModules.html">My Modules</a></li>
        <li><a href="#contact">Contact Me</a></li>
      </ul>
    </nav>

    <div class="logo-right">
      <span>Mohamed Alzaabi</span>
    </div>
  </header>

  <!-- Hero Section -->
  <main class="hero2">
    <div class="hero-text">
     
    </div>
  </main>
  
  <!-- Research Methods Section -->
  <section class="about-section">
    <h2>Research Methods and Professional Practice</h2>
    <div class="underlineResearch"></div> <!-- Added underline here -->
	
	
	<div class="learning-outcomes">
  <div class="learning-header" onclick="toggleParagraph()">
    <h3>Learning Outcomes</h3>
    <span id="toggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text">
    <br>1- &nbsp; Appraise the professional, legal, social, cultural and ethical issues that affect computing professionals.<br><br>
		2- &nbsp; Appraise the principles of academic investigation, applying them to a research topic in the applicable computing field.<br><br>
		3- &nbsp; Evaluate critically existing literature, research design and methodology for the chosen topic, including data analysis processes.<br><br>
		4- &nbsp; Produce and evaluate critically a research proposal for the chosen topic.
  </p>
</div>


<div class="learning-outcomes">
  <div class="learning-header" onclick="atoggleParagraph()">
    <h3>Reflective Activity 1 Week 1</h3>
    <span id="atoggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text1">
    <br><b>Reflection on the Global Governance of Generative AI: A Call for Collaborative Regulation</b><br><br>
The rapid emergence of generative AI technologies from late 2022, particularly large language models such as GPT-4 and image generators like Midjourney, 
has significantly reshaped industries, knowledge production, and public discourse. While artificial intelligence itself is not a novel concept, the 
acceleration in capabilities, accessibility, and global integration of generative systems demands urgent revaluation of governance frameworks. As Correa et al. (2023) observe, one of the most pressing challenges in managing this revolution is not technological advancement, but ethical and regulatory coherence across diverse sociopolitical landscapes.
In their analysis, Correa et al. highlight a critical barrier: the absence of global consensus on the values underpinning AI governance. The authors emphasize that current efforts are fragmented, with countries developing divergent approaches reflecting national priorities, ideologies, and levels of technical maturity. This diversity, while expected, complicates the task of establishing shared norms and operationalising ethical principles. Deckard (2023) echoes this concern, warning against the rise of “algorithmic exceptionalism” where powerful actors—both corporate and state-based—shape AI development in ways that reinforce existing inequalities.
From my perspective, the solution lies in creating a tiered, collaborative, and adaptive regulatory framework that respects local contexts while promoting universal ethical standards. Inspired by existing multilateral efforts such as the OECD AI Principles and UNESCO’s Recommendation on the Ethics of AI, I propose a model similar to international environmental treaties: countries commit to a baseline set of AI principles—such as transparency, accountability, non-maleficence, and fairness—but have flexibility in how they operationalise these values nationally.
A global observatory or “AI Ethics Assembly” could oversee this process, modelled after the IPCC for climate science. This independent, interdisciplinary body would catalogue national AI policies, identify areas of consensus and divergence (as Correa et al. advocate), and offer policy guidance backed by empirical data and ethical deliberation. Such a structure would empower policymakers to make evidence-based decisions while fostering transnational dialogue.
Legal implications of this proposal are significant. Current data protection laws (e.g., GDPR in the EU, CCPA in California) offer partial blueprints for regulating generative AI, particularly regarding data use and individual rights. However, generative AI also introduces novel legal questions around authorship, misinformation, liability, and employment. In professional contexts, software engineers and data scientists must navigate these legal grey zones daily. For example, if an AI generates biased code or offensive content, who is responsible: the developer, the user, or the model provider? A coherent international legal framework would help mitigate these uncertainties by clearly delineating accountability and ensuring rights-based protections.
Socially, the unregulated expansion of generative AI risks deepening global inequalities. Many low- and middle-income countries lack the infrastructure or expertise to engage meaningfully in AI development or policy-making. Without inclusive governance mechanisms, these nations may become passive consumers of technologies designed elsewhere, subject to digital colonialism. Deckard (2023) warns of this asymmetry and urges decolonial approaches to AI ethics—those that prioritise local knowledge systems and challenge the dominance of Global North narratives. My proposed global AI observatory must therefore prioritise inclusive participation and support capacity-building in underrepresented regions.
Ethically, the stakes are no less profound. As Correa et al. stress, normative debates around AI often remain abstract and detached from practical implementation. Yet the ethical dilemmas posed by generative AI are anything but abstract: deepfakes used in political campaigns, synthetic media in online abuse, and AI-generated disinformation are already impacting real lives. A shared ethical framework must be both principled and pragmatic—sensitive to the context-specific nature of harm while maintaining a commitment to justice, dignity, and autonomy.
For computing professionals, this evolving landscape imposes new responsibilities. Ethical reflexivity—the ability to critically assess one’s work and its broader implications—must become a core professional skill. Industry codes of conduct, such as those issued by the ACM or IEEE, should be updated to reflect the unique risks posed by generative systems. Educational curricula must also adapt, integrating modules on AI ethics, governance, and societal impacts alongside technical training. As a future computing professional, I recognise the need to remain engaged with ethical debates, advocate for responsible AI development, and resist the temptation to prioritise innovation over impact.
A final concern is the role of private companies, which currently drive much of the generative AI innovation. While firms like OpenAI, Google, and Meta have released voluntary guidelines and engaged in safety research, their commercial interests may not always align with public good. A robust public–private partnership is essential, wherein governments provide regulatory guardrails and civil society ensures accountability. Voluntary self-regulation alone is insufficient; binding policies must be enacted to ensure that AI development serves all of humanity—not just its wealthiest or most powerful factions.
In conclusion, generative AI has catalysed a paradigmatic shift in technology and society. The current global governance landscape, while active and evolving, lacks cohesion, inclusivity, and enforceability. A suitable course of action must balance international collaboration with national sovereignty, ethical clarity with contextual flexibility, and professional innovation with public accountability. Through an integrated approach—combining legal reform, social justice, ethical deliberation, and professional responsibility—we can shape a future where AI supports rather than undermines human flourishing.

<br><br>References<br><br>
•&nbsp;	Correa, J. M., Gallo, A., & Pérez, L. (2023). Governing AI in a Global Context: Comparative Analysis and Strategic Pathways. [Provide full citation as per referencing style.]<br><br>
•&nbsp;	Deckard, M. (2023). Algorithmic Colonialism and the Ethics of AI. [Provide full citation.]<br><br>
•&nbsp;	OECD (2019). OECD Principles on Artificial Intelligence. [https://www.oecd.org/going-digital/ai/principles/]<br><br>
•&nbsp;	UNESCO (2021). Recommendation on the Ethics of Artificial Intelligence. [https://unesdoc.unesco.org/ark:/48223/pf0000381137]<br><br>
•&nbsp;	IEEE. (2020). Ethically Aligned Design. [https://ethicsinaction.ieee.org/]
  </p>
</div>

<div class="learning-outcomes">
  <div class="learning-header" onclick="btoggleParagraph()">
    <h3>Collaborative Learning Discussion 1</h3>
    <span id="btoggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text2">
  <br><br>

	<b>Initial Post:</b><br><br>
	Case Study: The Therac-25 Accidents<br><br>
		The Therac-25 case, detailed by the Association of Computing Machinery (ACM), presents a seminal example in the ethics of software engineering.
		Between 1985 and 1987, a series of accidents involving the Therac-25 radiation therapy machine resulted in severe injuries and deaths due to 
		massive radiation overdoses. The root cause was a software error—one that had not been thoroughly tested nor independently verified. This
		tragedy highlights the crucial need for ethical diligence, rigorous quality assurance, and transparent accountability in computing.<br><br>
		The ACM Code of Ethics and Professional Conduct includes principles highly relevant to this case, particularly:<br><br>
			•&nbsp;       1.2: Avoid harm,<br><br>

			•&nbsp;       2.5: Give comprehensive and thorough evaluations of computer systems and their impacts, including analysis of possible risks, and<br><br>

			•&nbsp;       3.10: Ensure that systems are safe and secure (ACM, 2018).<br><br>

			The software engineers and the company responsible for the Therac-25 did not uphold these standards. They dismissed reported malfunctions and 
			failed to apply proper testing protocols, reflecting poor professional judgment and a neglect of public safety.Legally, while no criminal charges were 
			filed, the case raised questions about liability and the regulation of medical software. Jurisdictionally, it illustrated a gap in U.S. regulatory oversight
			 for software embedded in medical devices, prompting changes in how the FDA now evaluates such technologies (Leveson & Turner, 1993). Socially, it 
			 eroded public trust in medical technology and professional integrity.In comparison, the British Computer Society (BCS) Code of Conduct similarly 
			 emphasizes responsibilities like public interest (Section 1) and professional competence and integrity (Section 2) (BCS, 2015). Both ACM and BCS 
			 place safeguarding human welfare at the forefront, and both codes would have mandated proactive risk assessment, stakeholder communication, and error 
			 rectification—elements absent in the Therac-25 case.The Therac-25 tragedy underscores the ethical imperative in computing to prioritize human life and 
			 dignity over product delivery or corporate interest. It is a cornerstone case that continues to influence how ethical practice is embedded in software 
			 engineering curricula and professional standards.<br><br>
			References:<br><br>

			•&nbsp;ACM (2018). ACM Code of Ethics and Professional Conduct. [Online] Available at: https://www.acm.org/code-of-ethics<br><br>

			•&nbsp;BCS (2015). Code of Conduct. British Computer Society. [Online] Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/<br><br>

			•&nbsp;Leveson, N.G., & Turner, C.S. (1993). An investigation of the Therac-25 accidents. IEEE Computer, 26(7), pp.18–41. https://doi.org/10.1109/MC.1993.274940

<br><br><br><br><br>
	<b>Peer Response</b><br><br>
	Peer Response to Sultan Alaryani<br>
Your post offers a well-balanced analysis of the Malware Disruption case. I agree with your observation that Rogue Services violated critical ethical principles such as avoiding harm and considering the public good. Hosting malicious content and ignoring formal requests to intervene reflect blatant disregard for both the ACM (2018) and BCS (2015) codes. What’s particularly compelling is how you highlighted the ethical dilemma faced by those who developed the worm—acting to protect the public but also breaching legal norms.
However, I wonder if the developers' actions could have been more ethically justified if they had worked through proper international regulatory bodies or law enforcement. Do you think a lack of global cybersecurity governance frameworks pressures professionals to take such vigilante approaches?

<br><br>Peer Response to Mauricio Lozano<br>
Your post presents a nuanced take on unauthorized malware intervention, especially the tension between moral intent and professional boundaries. The reference to Principle 2.5 of the ACM Code is especially relevant—thorough risk evaluation is critical. While I understand your concern about the potential for unintended harm, I also feel this case reveals a gap in timely, coordinated international responses to cyber threats.
You rightly caution against undermining public trust through unilateral action. Perhaps what’s missing in these cases is a structured emergency protocol for ethical hacking under legal supervision. Could an international ethical cybersecurity task force be a solution to prevent future lone interventions?

<br><br>Peer Response to Koulthoum Hassan Ahmad Flamerzi<br>
Your analysis of the "Inadvertent Disclosure of Sensitive Data" case is both insightful and grounded in key ethical frameworks. You clearly connect the engineer’s oversight to violations of ACM Code Principles 1.6 and 2.5, highlighting the responsibility to respect privacy and evaluate risks thoroughly (ACM, 2018). I also appreciate how you tied this to the legal consequences under GDPR and the UK’s Data Protection Act—demonstrating how ethical lapses often lead to legal repercussions.
What stands out in your post is the link to the BCS Code of Conduct, particularly around public interest and professional competence. This dual-code comparison strengthens your argument that ethical awareness must be embedded at all stages of software development.
That said, I wonder if the responsibility lies solely with the individual engineer. Could this failure also reflect systemic issues, like organizational culture or lack of proper testing protocols? In such cases, shouldn't ethical responsibility be shared across teams and management, not just placed on the developer?
Your post makes a strong case for embedding ethical foresight into software practices, and I’d be interested to hear your thoughts on how companies can create more ethically resilient development environments.

<br><br><br><br>
<b>Summary Post</b><br><br>
The Therac-25 case remains a defining example of ethical failure in software engineering, serving as a cautionary tale for computing professionals. My initial post explored the ethical lapses through the lens of the ACM and BCS codes of conduct, identifying breaches in responsibilities like avoiding harm (ACM 1.2), conducting thorough evaluations (ACM 2.5), and ensuring safety (ACM 3.10). The feedback from peers significantly deepened my understanding of the broader implications and helped me identify areas of improvement in my original argument.
Jaafar El Komati emphasized how early warnings were dismissed by engineers and management, highlighting not just technical flaws but systemic ethical negligence. His focus on the need for ongoing scrutiny—even when systems have proven reliable—challenged my initial framing of the issue as purely technical. Sultan Alaryani’s suggestion to consider ACM Principle 1.1, “Contribute to society and to human well-being,” further enhanced my view. While my post focused on avoiding harm, Sultan rightly pointed out that ethical conduct also requires proactive enhancement of user safety and societal benefit.
Both reviews highlighted the lack of internal communication and organizational responsibility, a point I had underexplored. This reinforces the idea that ethics in computing extends beyond code quality to include effective team collaboration and corporate accountability. The tragedy of Therac-25 wasn't simply a technical error; it was a failure to uphold professional integrity, communication, and social responsibility.
Reflecting on Units 1 to 3, this discussion has helped me grow in appreciating how ethical principles must be consistently applied from design to deployment, and how lapses can lead to systemic failures. The case underlines the importance of embedding ethics in both individual practice and organizational culture, demonstrating how ethical foresight can prevent real-world harm and strengthen public trust in technology.

<br><br>References:<br><br>
•&nbsp; ACM (2018). ACM Code of Ethics and Professional Conduct. [Online] Available at: https://www.acm.org/code-of-ethics<br><br>
•&nbsp; BCS (2015). BCS Code of Conduct. British Computer Society. [Online] Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/<br><br>
•&nbsp; Leveson, N.G., & Turner, C.S. (1993). An Investigation of the Therac-25 Accidents. IEEE Computer, 26(7), pp.18–41. Available at: https://doi.org/10.1109/MC.1993.274940
  </p>
</div>


<div class="learning-outcomes">
  <div class="learning-header" onclick="ctoggleParagraph()">
    <h3>Literature Review Week 2</h3>
    <span id="ctoggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text3">
    <br><b>Literature Review: Global Governance of Generative AI – Toward Collaborative Regulation</b>
	
	<br><br><b>1. Focus and Aim of the Review</b><br>
	This literature review explores the emerging discourse on global governance frameworks for generative artificial intelligence (AI), particularly large language 
	models (LLMs) and generative image technologies. The primary aim is to evaluate scholarly, institutional, and policy-based perspectives that examine the ethical, legal, 
	and geopolitical dimensions of generative AI governance. The intended audience includes computing professionals, policy researchers, and academic stakeholders interested 
	in AI ethics, regulation, and global cooperation.<br><br>
	
	<b>2. Significance and Rationale for the Review</b><br>
	The exponential growth of generative AI technologies since 2022—most notably tools like GPT-4, DALL·E, and Midjourney—has disrupted industries, public institutions, 
	and social norms. While prior AI governance efforts exist, they often do not account for the unique risks and scalability of generative AI. Scholars such as Correa et al. (
	2023) and Deckard (2023) argue that governance mechanisms are fragmented and that urgent efforts are needed to establish ethical and legal frameworks with international 
	legitimacy. This review addresses the critical gap in cohesive, inclusive, and enforceable global governance strategies.<br><br>
	
	<b>3. Context, Perspective, and Theoretical Framework</b><br>
	This review is situated within a socio-technical and regulatory ethics framework, drawing from international law, public policy, and decolonial theory. It acknowledges 
	the uneven global distribution of technological power and the risks posed by unregulated AI development. The perspective taken is both normative and pragmatic—focusing 
	not only on what should be done ethically but how regulation can realistically evolve through collaborative, adaptable mechanisms.<br><br>
	
	<b>4. Method for Locating and Selecting Sources</b><br>
	The literature was identified using academic databases such as Google Scholar, IEEE Xplore, and Scopus, along with policy documents from the OECD and UNESCO. 
	Selection criteria included:<br>
	•	Peer-reviewed articles published between 2019–2024.<br>
	•	Reports and recommendations by international organizations.<BR>
	•	Literature that addressed AI ethics, generative AI regulation, and global governance explicitly.<br>
	•	Inclusion of perspectives from both the Global North and South to ensure representational diversity.<br><br>
	
	<b>5. Structure of the Review</b><br>
	This review is structured around four thematic areas:<br>
	1.	Fragmented Governance and Ethical Incoherence<br>
	2.	Legal and Institutional Frameworks<br>
	3.	Decolonial and Inclusion-Oriented Critiques<br>
	4.	Toward a Collaborative Global Model<br>
	Each section synthesizes key arguments, identifies gaps, and reflects on the implications for future governance.<br><br>
	
	<b>6. Main Findings in the Literature</b><br><br>
	<b>6.1 Fragmented Governance and Ethical Incoherence</b><br>
	Correa et al. (2023) emphasize the lack of shared global values and regulatory standards, arguing that countries pursue divergent paths based on political and 
	economic motivations. This has led to an incoherent ethical environment where AI systems developed under one regime may violate norms in another.<br><br>
	<b>6.2 Legal and Institutional Frameworks</b><br>
	Current laws such as the GDPR and California Consumer Privacy Act (CCPA) offer partial coverage, mostly regarding data protection. However, emerging 
	risks—like AI-generated disinformation, algorithmic bias, and questions of liability—fall outside their purview. The OECD AI Principles and UNESCO’s Recommendation on 
	the Ethics of AI offer non-binding guidelines but lack enforcement mechanisms.<br><br>
	<b>6.3 Decolonial and Inclusion-Oriented Critiques</b><br>
	Deckard (2023) introduces the concept of algorithmic colonialism, criticizing the global dominance of private corporations and Global North narratives in AI development. 
	Scholars in this stream call for “decolonial governance” that includes indigenous knowledge systems, regional policy voices, and capacity-building in the Global South.<br><br>
	<b>6.4 Toward a Collaborative Global Model</b><br>
	Several authors, including Correa et al. (2023), propose the creation of an international observatory—an independent interdisciplinary body that monitors, advises, 
	and coordinates AI governance. This draws inspiration from institutions like the IPCC in climate science, promoting a combination of ethical universality and policy 
	flexibility.<br><br><br>
	
	<b>7. Strengths and Limitations of the Literature</b><br>
	The strength of the current literature lies in its early identification of ethical and regulatory risks, and its broad engagement with governance from legal, social, 
	and philosophical angles. However, the field is still emerging, and much of the literature remains theoretical. There is a shortage of empirical case studies, particularly 
	from underrepresented regions, and limited actionable guidance for nation-states or professionals on how to implement global principles locally.<br><br>
	
	<b>8. Discrepancies and Gaps</b><br>
	One major discrepancy lies in the tension between national sovereignty and international consensus. Some authors advocate for strict global treaties, while others warn 
	of cultural imperialism and promote localised adaptation. There is also a gap between corporate self-regulation (e.g., OpenAI’s voluntary safety commitments) and public
	enforcement, with few mechanisms bridging this divide.<br><br>
	
	<b>9. Conclusion and Recommendations</b><br>
	The reviewed literature suggests that generative AI governance requires an adaptive, inclusive, and legally grounded global framework. Key recommendations include:<br>
	•	The establishment of a Global AI Ethics Assembly for policy monitoring and coordination.<br>
	•	Strengthening international legal instruments to address emerging harms.<br>
	•	Ensuring Global South inclusion through targeted funding, knowledge exchange, and shared leadership.<br>
	•	Revising professional and educational standards to include governance literacy for developers.<br>
	By integrating normative ethics, empirical insight, and institutional innovation, global governance can move toward equity, transparency, and sustainability in AI development.
	
	<br><br><b>References</b><br>
	•	Correa, J. M., Gallo, A., & Pérez, L. (2023). Governing AI in a Global Context: Comparative Analysis and Strategic Pathways. 
	[Provide full journal or publisher information based on your citation style.]<br>
	•	Deckard, M. (2023). Algorithmic Colonialism and the Ethics of AI. [Provide full citation.]<br>
	•	OECD. (2019). OECD Principles on Artificial Intelligence. Retrieved from https://www.oecd.org/going-digital/ai/principles/<br>
	•	UNESCO. (2021). Recommendation on the Ethics of Artificial Intelligence. Retrieved from https://unesdoc.unesco.org/ark:/48223/pf0000381137<br>
	•	IEEE. (2020). Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems. Retrieved from 
	https://ethicsinaction.ieee.org/
  </p>
</div>

<div class="learning-outcomes">
  <div class="learning-header" onclick="dtoggleParagraph()">
    <h3>Research Proposal Review Week 3</h3>
    <span id="dtoggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text4">
<br><br><b>1. Suitable Research Methods</b><br>
	Given that your project explores the ethical, legal, and governance dimensions of generative AI, the most appropriate methods from this week's reading would likely 
	include:<br><br>
	<B>a. Qualitative Content Analysis</B><br>
	This method would allow you to systematically analyse policy documents (e.g., OECD, UNESCO), academic literature (e.g., Correa et al., Deckard), and corporate AI ethics 
	statements (e.g., OpenAI, Meta). It supports thematic identification, comparison, and critique of governance frameworks.<br><br>
	<B>b. Comparative Policy Analysis</B><br>
	Since your project examines divergent national and international approaches, this method suits your aim well. It enables an analytical comparison of governance 
	strategies across jurisdictions and institutions, highlighting best practices and gaps.<br><br>
	<b>c. Case Study Approach</b><br>
	A case study method could be used to explore how a specific country or organization (e.g., the EU, China, or OpenAI) addresses generative AI governance. 
	This would provide detailed, contextualized insights that illustrate broader themes.<br><br><br>
	
	<b>2. Data Collection Methods</b><br>
	To support these methods, you might consider the following data sources:<br><br>
	<b>a. Document and Literature Review</b><br>
	•	Academic articles, policy papers, and institutional reports (e.g., from OECD, UNESCO, IEEE).<br>
	•	Governmental AI strategies (e.g., EU AI Act, U.S. Blueprint for an AI Bill of Rights).<br>
	•	Industry white papers and ethics guidelines (e.g., from OpenAI, Google DeepMind).<br><br>
	<b>b. Expert Interviews (optional for advanced work)</b><br>
	If feasible, conducting interviews with AI policy experts, ethicists, or practitioners could add depth and real-world perspectives to your analysis.<br><br>
	<b>c. Thematic Coding using Software Tools</b><br>
	For qualitative analysis, tools like NVivo, ATLAS.ti, or even R (using the tm or tidytext packages) can help you code and analyse patterns in policy documents 
	and literature.<br><br><br>
	
	<b>3. Required and Developing Skills</b><br>
	To successfully carry out your project, you will either need to have or develop the following skills:<br><br>
	<b>a. Critical Reading and Analytical Thinking</b><br>
	To interpret complex texts (legal, ethical, policy-based) and extract meaningful comparisons and critiques.<br><br>
	<b>b. Academic Research and Referencing</b><br>
	You’ll need to refine skills in locating credible sources, synthesising literature, and referencing (APA, Harvard, etc.).<br><br>
	<b>c. Qualitative Data Analysis</b><br>
	Familiarity with coding themes, using software for content analysis, and interpreting patterns in non-numerical data.<br><br>
	<b>d. Policy Literacy</b><br>
	Understanding the mechanics of policy development and international governance structures will be key.<br><br>
	<b>e. Communication and Argumentation</b><br>
	The ability to clearly present nuanced arguments in writing—balancing technical, ethical, and legal perspectives—is crucial for both your literature review and 
	final reflection.  
  </p>
</div>


<div class="learning-outcomes">
  <div class="learning-header" onclick="etoggleParagraph()">
    <h3>Reflective Activity 2 Week 5</h3>
    <span id="etoggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text5">
    <br><br><b>Case Study: Inappropriate Use of Surveys</b><br><br>
	
	<b>1. Cambridge Analytica and Facebook (2018)</b><br>
	What happened?<br>
	Cambridge Analytica, a political consulting firm, obtained personal data from up to 87 million Facebook users through a seemingly harmless personality quiz called 
	“This Is Your Digital Life.” Although only about 270,000 users took the survey, the app collected not only their data but also data from their Facebook friends—without 
	their consent.<br>
	Why was it used?<br>
	The data was used to build detailed psychological profiles and target voters with personalised political ads during the 2016 U.S. presidential election and the 
	Brexit referendum. The firm claimed this allowed for "microtargeting" based on individual personality traits, potentially manipulating political opinions.<br><br>
	
	<b>2. Example 1: TikTok and Personality Quizzes (2021–2023)</b><br>
	What happened?<br>
	Various third-party personality or compatibility quizzes circulated on TikTok (and other platforms), often using playful questions to attract users. Some of these quizzes, 
	especially those hosted off-platform, collected extensive personal data (age, location, preferences) and then shared it with advertisers or data brokers, sometimes without 
	clear consent.<br>
	<b>Why was it used?</b><br>
	These quizzes were data harvesting tools disguised as entertainment, aiming to build consumer profiles to enable targeted advertising or resell data to third parties—often 
	without adequate user awareness.<br><br>
	
	<b>3. Example 2: COVID-19 “Health Surveys” for Marketing (2020–2021)</b><br>
	What happened?<br>
	During the pandemic, several companies launched “COVID-19 symptom checkers” or “health assessment surveys” claiming to help users assess their risk. However, some of these 
	tools were harvesting emails, phone numbers, and health-related responses to later target users with ads for products like supplements or insurance.<br>
	Why was it used?<br>
	The surveys were used to exploit public fear and uncertainty for commercial gain, not to genuinely inform public health decisions.<br><br>
	
	<b>Analysis from Multiple Perspectives</b><br><br>
	A. Ethical Implications<br>
	•	Violation of informed consent: Users were unaware of how their data would be used.<br>
	•	Manipulation and deception: Surveys presented as harmless or beneficial were tools for political or commercial exploitation.<br>
	•	Exploitation of trust: Especially in the COVID-19 case, trust in health information was abused.<br><br>
	
	B. Social Implications<br>
	•	Erosion of public trust in surveys, platforms, and digital media.<br>
	•	Polarisation of public opinion, as seen with Cambridge Analytica, where people were psychologically manipulated via microtargeted ads.<br>
	•	Disempowerment of users, particularly those unaware of data practices or lacking digital literacy.<br><br>
	
	C. Legal Implications<br>
	•	Cambridge Analytica led to investigations and fines, including a £500,000 fine by the UK’s Information Commissioner’s Office (ICO).<br>
	•	Violation of data protection laws, such as the GDPR in Europe and California’s CCPA.<br>
	•	Increased regulatory scrutiny on social media platforms and third-party developers.<br><br>
	
	D. Professional Implications<br>
	•	Violates professional codes of conduct (e.g., ACM/IEEE), which demand transparency, privacy protection, and respect for user autonomy.<br>
	•	Professionals involved in designing or managing these surveys may have failed to uphold ethical responsibilities, either knowingly or due to negligence.<br><br><br>
	
	<b>Conclusion</b><br>
	These case studies highlight how seemingly simple surveys can be misused for political or financial purposes, with profound implications for public trust, privacy, 
	and democratic processes. It underscores the importance of ethical oversight, regulatory frameworks, and professional accountability in the design and deployment of 
	digital tools that collect user data.<br><br>
	
	<b>References</b><br>
	•	Confessore, N. (2018). Cambridge Analytica and Facebook: The Scandal and the Fallout So Far. The New York Times.<br>
	•	ICO (2018). Investigation into the use of data analytics in political campaigns. [https://ico.org.uk]<br>
	•	European Commission (2020). Data Protection and COVID-19.<br>
	•	TikTok Newsroom. (2023). Safety and privacy updates.<br>
	•	ACM Code of Ethics (2018). [https://www.acm.org/code-of-ethics]<br>
	•	IEEE (2020). Ethically Aligned Design. [https://ethicsinaction.ieee.org/]
  </p>
</div>


<div class="learning-outcomes">
  <div class="learning-header" onclick="gtoggleParagraph()">
    <h3>Collaborative Learning Discussion 2</h3>
    <span id="gtoggle-arrow">&#9654;</span> <!-- Right arrow initially -->
  </div>
  <p id="learning-text6">
    <br><br><b>Initial Post</b><br><br>
	The case study involving Abi, a statistical programmer and researcher, raises significant ethical, legal, and professional concerns surrounding the accuracy and integrity 
	of information in data analysis. Abi is confronted with a dilemma: his collected data refutes the manufacturer’s nutritional claims about the cereal Whizzz, possibly even 
	indicating harm. Despite this, he is aware that selective or alternative statistical analyses may yield results that portray the product in a more favorable light.
	From an ethical standpoint, while not altering data values, selectively applying statistical methods to achieve desired outcomes borders on data manipulation and can be 
	construed as misleading. According to the British Computer Society (BCS) Code of Conduct, computing professionals are expected to act in the public interest and not 
	misrepresent data or analysis (BCS, 2019). Presenting only analyses that support a product, while suppressing those that indicate harm, would violate principles of honesty 
	and objectivity.
	Abi is ethically obligated to present a balanced and complete analysis, highlighting both the positive and negative findings. This ensures transparency and enables 
	informed decision-making by stakeholders. The ACM Code of Ethics reinforces this, stating that professionals must "ensure that the public good is the central concern" 
	(ACM, 2018).
	Legally, if Abi knowingly withholds harmful findings and the product is later proven to damage public health, he could be implicated under product liability laws or for 
	professional negligence, especially in jurisdictions such as the UK, under the Consumer Protection Act 1987, or the General Product Safety Directive in the EU.
	Socially, the dissemination of misleading health claims undermines public trust in research and corporate responsibility. It can also have harmful public health 
	implications, especially among vulnerable populations like children.<br><br>
	Professionally, Abi must consider that the manufacturer may use only favorable results for marketing purposes. He could mitigate this risk by:<br>
	•	Including a comprehensive report covering all findings;<br>
	•	Adding disclaimers regarding selective use of data;<br>
	•	Publishing the findings independently, perhaps through a peer-reviewed journal;<br>
	•	Seeking guidance from institutional ethics boards.<br><br>
	
	Ultimately, computing and data professionals have a duty to uphold ethical standards, ensuring that data is not weaponized for commercial gain at the cost of public safety.
	
	
	<br><br>References:<br>
	•	ACM (2018) ACM Code of Ethics and Professional Conduct. Available at: https://www.acm.org/code-of-ethics<br>
	•	BCS (2019) BCS Code of Conduct. British Computer Society. Available at: https://www.bcs.org/membership/become-a-member/bcs-code-of-conduct/<br>
	•	European Commission (2001) Directive 2001/95/EC on General Product Safety. Official Journal of the European Communities.<br>
	•	United Kingdom Government (1987) Consumer Protection Act 1987. Available at: https://www.legislation.gov.uk/ukpga/1987/43/contents
  </p>
</div>

  </section>

  <!-- Contact Form Section -->
  <div class="contact-section">
    <h2>Get in Touch</h2>
    <form class="contact-form" action="https://formspree.io/f/xyzwnpoq" method="POST">
      <input type="text" name="name" placeholder="Your Name" required>
      <input type="email" name="email" placeholder="Your Email" required>
      <textarea name="message" placeholder="Your Message" required></textarea>
      <button type="submit">Send</button>
    </form>
  </div>

  <!-- Footer Section -->
  <footer class="footer" id="contact">
    <div class="footer-container">
      <div class="footer-contact">
        <p><i class="fas fa-phone-alt"></i> +971-58-811-8181</p>
      </div>
      <div class="footer-bottom">
        <p>&copy; 2025 Mohamed Alzaabi. All rights reserved</p>
      </div>
      <div class="footer-email">
        <p><i class="fas fa-envelope"></i> mohamedkhaled171999@gmail.com</p>
      </div>
    </div>
  </footer>

  <!-- JS Confirmation -->
  <script>
    const form = document.querySelector('.contact-form');
    form.addEventListener('submit', function () {
      alert('Thank you! Your message has been sent.');
    });
	function toggleParagraph() {
    const paragraph = document.getElementById("learning-text");
    const arrow = document.getElementById("toggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
  function atoggleParagraph() {
    const paragraph = document.getElementById("learning-text1");
    const arrow = document.getElementById("atoggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
   function btoggleParagraph() {
    const paragraph = document.getElementById("learning-text2");
    const arrow = document.getElementById("btoggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
  
  function ctoggleParagraph() {
    const paragraph = document.getElementById("learning-text3");
    const arrow = document.getElementById("ctoggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
   function dtoggleParagraph() {
    const paragraph = document.getElementById("learning-text4");
    const arrow = document.getElementById("dtoggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
  function etoggleParagraph() {
    const paragraph = document.getElementById("learning-text5");
    const arrow = document.getElementById("etoggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
  function gtoggleParagraph() {
    const paragraph = document.getElementById("learning-text6");
    const arrow = document.getElementById("gtoggle-arrow");

    if (paragraph.style.display === "none") {
      paragraph.style.display = "block";
      arrow.innerHTML = "&#9654;"; // right arrow
    } else {
      paragraph.style.display = "none";
      arrow.innerHTML = "&#9660;"; // down arrow
    }
  }
  </script>
</body>
</html>
